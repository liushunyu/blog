---

layout:     post
title:      "强化学习论文（4）BiCNet"
subtitle:   "Multiagent Bidirectionally-Coordinated Nets Emergence of Human-level Coordination in Learning to Play StarCraft Combat Games"
date:       2020-06-06 09:00:00
author:     Shunyu
header-img: img/post-bg-2015.jpg
header-mask: 0.1
catalog: true
mathjax: true
tags:
    - 强化学习
    - 强化学习论文
---



**标签：**

BiCNet; actor-critic; off-policy; model-free; communication; continuous communication channel; continuous action space; continuous state space; cooperative task; centralized approach; multi-agent;



[论文链接](https://arxiv.org/abs/1703.10069)



## 创新点及贡献

1、通过一个基于双向 RNN 网络的 deterministic actor-critic 结构来学习多智能体之间的通信协议，其中双向 RNN 网络的序列长度为智能体数目，通信信息通过 RNN 的隐藏状态进行传递。

2、证明了 Multiagent Deterministic policy gradient Theorem。



## 研究痛点

1、大规模多智能体学习面临一个重大挑战，即随着所涉及的智能体数量的增加，参数空间呈指数增长，以至于联合所有智能体同时训练学习的方法的效率将变得十分低下。



## 算法流程

框架图如下

<img width="60%" src="/img/in-post/2020-06-06-强化学习论文（4）BiCNet.assets/image-20200606210425052.png"/>



### 主要思路

1、论文一开始将游戏定义为一个两队智能体的零和博弈问题，然后为了方便定义了敌人的策略是固定的，这便变成了一个完全协作问题。



2、整个模型是一个基于双向 RNN 网络的 deterministic actor-critic 结构，其中双向 RNN 网络的序列长度为智能体数目，所以其实每个智能体的参数是共享的，通信信息将通过 RNN 的隐藏状态进行传递。

- 注意在 RNN 结构中通信结构不是完全对成的，所以每个智能体角色的顺序是需要预先定义好的。
- 其中所有的智能体共享同样的观察状态。



3、reward 定义如下

<img height="50%" src="/img/in-post/2020-06-06-强化学习论文（4）BiCNet.assets/image-20200606212252672.png"/>



其中第一项正比于敌方上一时刻到这一时刻被扣的血量，第二项正比于我方上一时刻到这一时刻被扣的血量。



4、贝尔曼最优方程定义如下

<img height="50%" src="/img/in-post/2020-06-06-强化学习论文（4）BiCNet.assets/image-20200606212315154.png"/>



5、证明了 Multiagent Deterministic policy gradient Theorem，Actor 网络的梯度定义如下

<img height="50%" src="/img/in-post/2020-06-06-强化学习论文（4）BiCNet.assets/image-20200606212013035.png"/>



6、Critic 网络的梯度定义如下

<img height="50%" src="/img/in-post/2020-06-06-强化学习论文（4）BiCNet.assets/image-20200606212119580.png"/>



### 算法伪代码

BiCNet 算法伪代码如下

<img width="100%" src="/img/in-post/2020-06-06-强化学习论文（4）BiCNet.assets/image-20200606210735759.png"/>



## 实验

1、采用了两个实验环境

- StarCraft combats：星际争霸中较多智能体互相对战的环境，其中敌人使用 built-in AI 的策略。

- simpler game：输入给每个智能体一个数字希望它们求和输出，用于测试智能体之间的通信是否含有语义信息。



2、实验结果

- 主要注意 BicNet 可以胜任较多智能体协作的场景问题。

<img width="90%" src="/img/in-post/2020-06-06-强化学习论文（4）BiCNet.assets/image-20200606213112203.png"/>



3、实验发现智能体学习到了多种人类级别的协作控制策略：Coordinated moves without collision、Hit and Run tactics、Coordinated cover attack、Focus fire without overkill



## 其他补充

1、论文的主体结构是在同构智能体上实现的，所以所有智能体的动作空间是一样的，但是论文还提出他们的框架同样可以应用在异构智能体上，每个智能体可以有各自的动作空间，主要通过限制网络结构中的参数共享实现。

- 论文在这里说的不是很清楚，但个人认为在 RNN 中所有智能体共享参数空间是一件必然的事情，不确定它们是怎么修改的，而且动作空间至少也得保证维度是一致的才行。



## 参考资料及致谢

所有参考资料在《强化学习思考（1）前言》中已列出，再次向强化学习的大佬前辈们表达衷心的感谢！

