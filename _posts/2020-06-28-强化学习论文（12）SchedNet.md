---

layout:     post
title:      "强化学习论文（12）SchedNet"
subtitle:   "Learning to Schedule Communication in Multi-agent Reinforcement Learning"
date:       2020-06-28 09:00:00
author:     Shunyu
header-img: img/post-bg-2015.jpg
header-mask: 0.1
catalog: true
mathjax: true
tags:
    - 强化学习
    - 强化学习论文
---



**标签：**

SchedNet; actor-critic; off-policy; model-free; communication; continuous communication channel; discrete action space; continuous state space; cooperative task; centralized training with decentralized execution; multi-agent;



[论文链接](https://arxiv.org/abs/1902.01554v1)



## 创新点及贡献

1、SchedNet 基于 actor-critic 设计了一种集中训练分散执行的网络架构，通过引入无线通信领域的 MAC（Medium Access Control）协议实现对智能体进行调度通信，从而保证在有限通信带宽的情况最高效率的进行通信。



## 研究痛点

1、目前在 MARL 领域仍未有人对受限智能体的调度问题进行研究，这其中主要包括两个问题:

- 一个是通信信道的带宽受限，有限的带宽意味着智能体必须学会交换简洁有效的信息
- 另一个是 shared medium 下智能体之间的通信冲突，所以必须对智能体的通信竞争进行适当的仲裁以避免冲突，因此需要进行通信调度。



## 算法流程

SchedNet 框架图

<img width="100%" src="/img/in-post/2020-06-28-强化学习论文（12）SchedNet.assets/image-20200628104617580.png"/>



### 主要思路

1、整体框架主要由三个部分组成：actor network、scheduler、critic network

- 注意其实这里 actor network 是由两个不同的 policy 组成的，其中一个 policy 是权重生成器，它的动作空间是连续的，因而本文使用了 DDPG 算法。另一个 policy 则是真正的 policy，它的动作空间是离散的，因而本文使用了普通的 actor-critic 算法。



2、每一个智能体都有自己独立的 actor 网络，输入的是局部观察，该网络由以下三部分组成：

- schedule vector $\mathbf{c}$ 是一个二值向量，表示哪个智能体被调度，其中 $\otimes$ 表示将所有被调度的智能体的信息进行 concatenation。

<img width="60%" src="/img/in-post/2020-06-28-强化学习论文（12）SchedNet.assets/image-20200628105301820.png"/>



3、scheduler 部分由一个 Weight- based Scheduling Algorithm 组成，WSA 采用了以下两种简单但实用的无线调度协议。

- 关于如何在分散执行的时候实现这两种调度协议，作者主要采用了一种叫做 CSMA（Carrier Sense Multiple Access）的分布式 MAC 调度器，其具体实现在附录中说明，不再本文讨论范围之内。

<img width="100%" src="/img/in-post/2020-06-28-强化学习论文（12）SchedNet.assets/image-20200628110423831.png"/>



4、critic 部分输入的是 global state 和权重动作，主要包括两个 heads

- 一个用于训练权重生成器的 policy，因为动作空间是连续的，因而本文使用了 DDPG 算法，所以需要 Q value。
- 一个用于训练真正的policy，它的动作空间是离散的，因而本文使用了普通的 actor-critic 算法，因而需要 state value。

<img width="50%" src="/img/in-post/2020-06-28-强化学习论文（12）SchedNet.assets/image-20200628111042162.png"/>



### 训练过程

1、Weight generators 网络求导

<img width="90%" src="/img/in-post/2020-06-28-强化学习论文（12）SchedNet.assets/image-20200628111617145.png"/>



2、Message encoders and action selectors 网络求导

<img width="90%" src="/img/in-post/2020-06-28-强化学习论文（12）SchedNet.assets/image-20200628111638690.png"/>



### 算法伪代码

SchedNet 算法伪代码

<img width="100%" src="/img/in-post/2020-06-28-强化学习论文（12）SchedNet.assets/image-20200628111709164.png"/>



## 实验

1、在两个协作环境上进行实验

- Predator and Prey
- Cooperative Com- munication and Navigation

<img width="100%" src="/img/in-post/2020-06-28-强化学习论文（12）SchedNet.assets/image-20200628111836977.png"/>



## 其他补充

1、其实本人有个疑惑，目前的通信带宽受限真的如此严重吗，只能支持几比特的数据进行传输，如果带宽受限没有那么严重，那么这些研究带宽受限的论文是否就没有那么大意义了。

2、论文中提出了一种在带宽受限下的通信调度算法，在基于其集中训练分散执行的框架，貌似很难运用在大规模智能体环境中，实验中也只进行了很简单的实验，那在小规模智能体环境中研究带宽受限感觉也没啥意义。

3、有一点点像 dropout，但毕竟是调度式的不是随机式的。



## 参考资料及致谢

所有参考资料在《强化学习思考（1）前言》中已列出，再次向强化学习的大佬前辈们表达衷心的感谢！

