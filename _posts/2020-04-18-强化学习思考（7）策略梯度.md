---
layout:     post
title:      "强化学习思考（7）策略梯度"
subtitle:    "策略梯度"
date:       2020-04-18 09:00:00
author:     Shunyu
header-img: img/post-bg-2015.jpg
header-mask: 0.1
catalog: true
mathjax: true
tags:
    - 强化学习
    - 强化学习思考
---



关于策略梯度的注意事项。



## 目录

- [强化学习思考（1）前言](https://liushunyu.github.io/2020/04/12/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E6%80%9D%E8%80%83-1-%E5%89%8D%E8%A8%80/)
- [强化学习思考（2）强化学习简介](https://liushunyu.github.io/2020/04/12/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E6%80%9D%E8%80%83-2-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/)
- [强化学习思考（3）马尔可夫决策过程](https://liushunyu.github.io/2020/04/12/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E6%80%9D%E8%80%83-3-%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B/)
- [强化学习思考（4）模仿学习和监督学习](https://liushunyu.github.io/2020/04/13/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E6%80%9D%E8%80%83-4-%E6%A8%A1%E4%BB%BF%E5%AD%A6%E4%B9%A0%E5%92%8C%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/)
- [强化学习思考（5）动态规划](https://liushunyu.github.io/2020/04/15/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E6%80%9D%E8%80%83-5-%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/)
- [强化学习思考（6）蒙特卡罗和时序差分](https://liushunyu.github.io/2020/04/15/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E6%80%9D%E8%80%83-6-%E8%92%99%E7%89%B9%E5%8D%A1%E7%BD%97%E5%92%8C%E6%97%B6%E5%BA%8F%E5%B7%AE%E5%88%86/)
- [强化学习思考（7）策略梯度](https://liushunyu.github.io/2020/04/18/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E6%80%9D%E8%80%83-7-%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6/)



## 基本定义

**Policy of actor $\pi_\theta(a \mid s)$**

Policy 可以理解为一个包含参数 $\theta$ 的神经网络，该网络将状态 state 作为模型的输入，输出对应行动 action 的概率分布。



**Trajectory $\tau$**

行动 action 和状态 state 的序列，表示为：


$$
\tau=\left\{s_{1}, a_{1}, s_{2}, a_{2}, \cdots, s_{T}, a_{T}\right\}
$$



**Probability $p_{\theta}(\tau)$**

给定神经网络参数 $\theta$ 的情况下，出现行动状态序列 $\tau$ 的概率为以下概率的乘积：初始状态出现的概率；给定当前状态，采取某一个行动的概率；以及采取该行动之后，基于该行动以及当前状态返回下一个状态的概率，用公式表示为：


$$
\begin{aligned}
p_{\theta}(\tau) &=p\left(s_{1}\right) \pi_{\theta}\left(a_{1} | s_{1}\right) p\left(s_{2} | s_{1}, a_{1}\right) \pi_{\theta}\left(a_{2} | s_{2}\right) p\left(s_{3} | s_{2}, a_{2}\right) \cdots
\\ &=p\left(s_{1}\right) \prod_{t=1}^{T} \pi_{\theta}\left(a_{t} | s_{t}\right) p\left(s_{t+1} | s_{t}, a_{t}\right)

\end{aligned}
$$



**The goal of reinforcement learning**

最大化期望总奖赏 reward：行动状态序列 $\tau$ 的概率分布下的期望总奖赏 reward。


$$
\theta^{\star} = \arg \max _{\theta} E_{\tau \sim p_{\theta}(\tau)}\left[\sum_{t} r\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\right]
$$


在Finite horizon case：


$$
\theta^{\star} =\arg \max _{\theta} \sum_{t=1}^{T} E_{\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right) \sim p_{\theta}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)}\left[r\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\right]
$$


在 Infinite horizon case： 


$$
\begin{aligned}
\theta^{\star}&=\arg \max _{\theta} \lim_{T \to \infty}\frac{1}{T} \sum_{t=1}^{T} E_{\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right) \sim p_{\theta}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)}\left[r\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\right] \\ &= \arg \max _{\theta} E_{(\mathbf{s}, \mathbf{a}) \sim p_{\theta}(\mathbf{s}, \mathbf{a})}[r(\mathbf{s}, \mathbf{a})]
\end{aligned}
$$


注意这里的 $p_{\theta}(\mathbf{s}, \mathbf{a})$ 需要是稳态分布



## 策略梯度 Policy Gradient

### 目标函数

定义期望总奖赏 reward 为模型的目标函数，其中通过采样的方法来近似期望。


$$
J(\theta) = E_{\tau \sim p_{\theta}(\tau)}\left[\sum_{t} r\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\right] = \int p_{\theta}(\tau)  r(\tau)d\tau

\\

\approx \frac{1}{N}\sum_i \sum_t r\left(\mathbf{s}_{i,t}, \mathbf{a}_{i,t}\right)

= \frac{1}{N}\sum_i r(\tau_i)
$$



其中


$$
r(\tau_i) = \sum_{t}  r\left({s}_{i,t}, {a}_{i,t}\right)
$$


### 求解梯度

得出目标函数之后，就需要根据目标函数求解目标函数最大值以及最大值对应的 policy 的参数 $\theta$，因此需要采取的方法是梯度上升。



$$
\begin{aligned}
&\nabla_\theta J(\theta) \\
=&\int \nabla_\theta p_{\theta}(\tau) r(\tau)  d\tau \\ 
=&\int  p_{\theta}(\tau) \frac{\nabla_\theta p_{\theta}(\tau)}{p_{\theta}(\tau)}r(\tau) d\tau\\
=&\int p_{\theta}(\tau) \nabla_\theta \log p_{\theta}(\tau) r(\tau)  d\tau \quad\fbox{1} \\
=&E_{\tau \sim p_{\theta}(\tau)}\left[ \nabla_\theta \log p_{\theta}(\tau)r(\tau)\right] \\
\approx &\frac{1}{N} \sum_{i=1}^{N}  \nabla_\theta \log p_{\theta}\left(\tau_{i}\right) r\left(\tau_{i}\right) \quad\fbox{2}
\\ 
=&\frac{1}{N} \sum_{i=1}^{N}  \left(\sum_{t=1}^{T} \nabla_\theta \log \pi_{\theta}\left(a_{i,t} | s_{i,t}\right) \right) \left(\sum_{t=1}^{T}   r\left({s}_{i,t}, {a}_{i,t}\right) \right)  \quad\fbox{3}
\end{aligned}
$$


注：

$\fbox{1}$ 因为 


$$
\nabla_\theta\log{p_{\theta}(\tau)}= \frac{1}{p_{\theta}(\tau)} \nabla_\theta p_{\theta}(\tau)
$$


此外使用 $\frac{\nabla p_{\theta}(\tau)}{p_{\theta}(\tau)}$ 这种方法还可以达到归一化的效果，通过除以每个 $\tau$ 出现的概率归一化每个 $\tau$ 出现的概率，这样就不会使算法更加倾向于提高出现概率高但总 reward 比较低的 $\tau$ 的概率，而是会提高总 reward 比较高的 $\tau$ 的概率。

$\fbox{2}$ 因为训练的过程中会进行采样训练，采样个数为 $N$，因此公式可以近似表示为 $N$ 次采样得到的 reward 的平均。

$\fbox{3}$ 因为


$$
{\log p_\theta(\tau)} {=\log p\left(s_{1}\right)+\sum_{t=1}^{T} \log \pi_\theta \left(a_{t} | s_{t}\right)+\log p\left(r_{t}, s_{t+1} | s_{t}, a_{t}\right)}
$$


$p_{\theta}(\tau)$ 中只有 $\pi_{\theta}\left(a_{t} \mid s_{t}\right)$ 项与 $\theta$ 有关，其他项相当于常数，求导之后就没了，而且导数项可以放在求和号里面。





### 梯度更新

使用当前的 $\pi_{\theta}$ 来采样多个回合的 $\tau$，然后使用采样得到的 $\tau$ 来更新 $\theta$，再用新的 $\pi_{\theta}$ 来重新采样多个回合的 $\tau$ 来更新 $\theta$。 


$$
\theta \leftarrow \theta+\eta \nabla \overline{R}_{\theta}
$$



1、sample $\tau_{i}$ from $\pi_{\theta}$ (run the policy)

2、compute the gradient


$$
\nabla_{\theta} J(\theta) \approx \frac{1}{N} \sum_{i=1}^{N} \left(\sum_{t=1}^{T} \nabla_\theta \log \pi_{\theta}\left(a_{i,t} \mid s_{i,t}\right) \right) \left(\sum_{t=1}^{T} r\left({s}_{i,t}, {a}_{i,t}\right) \right)
$$


3、$\theta \leftarrow \theta+\alpha \nabla_{\theta} J(\theta)$





## 策略梯度与最大化似然

- 策略梯度 Policy Gradient


$$
\nabla_{\theta} J_{PG}(\theta) \approx \frac{1}{N} \sum_{i=1}^{N}  \left(\sum_{t=1}^{T} \nabla_\theta \log \pi_{\theta}\left(a_{i,t} | s_{i,t}\right) \right) \left(\sum_{t=1}^{T}   r\left({s}_{i,t}, {a}_{i,t}\right) \right)
$$


- 最大化似然 Maximum Likelihood


$$
\nabla_{\theta} J_{ML}(\theta) \approx \frac{1}{N} \sum_{i=1}^{N}  \left(\sum_{t=1}^{T} \nabla_\theta \log \pi_{\theta}\left(a_{i,t} | s_{i,t}\right) \right)
$$


这两条式子从一定程度上说明了强化学习与监督学习的不同：

- 监督学习的样本标签是人工标注为“对”的，可以直接用来学习
  - 更新策略使得样本出现的概率越来越高
- 强化学习的样本“标签”是根据策略选出来的，需要辅助奖赏来进行学习
  - 强化学习是一种试错学习 tail and error
  - 更新策略使得奖赏大的样本出现的概率越来越高





## 算法改进

### Partial observability 

由于式子中没有利用到 Markov property，所以可以直接将策略梯度用于 Partially observed MDPs


$$
\nabla_{\theta} J_{PG}(\theta) \approx \frac{1}{N} \sum_{i=1}^{N}  \left(\sum_{t=1}^{T} \nabla_\theta \log \pi_{\theta}\left(a_{i,t} | o_{i,t}\right) \right) \left(\sum_{t=1}^{T}   r\left({s}_{i,t}, {a}_{i,t}\right) \right)
$$


- 根据前面的推导，即使 $p_\theta(\tau)$ 中包含 $p(o \mid s)$，在求解 $log$ 并求导之后也会消掉。



### High Variance

首先简化一下式子方便我们分析：


$$
\nabla_{\theta} J_{PG}(\theta) = E_{\tau \sim \pi_\theta (\tau)} [\nabla_\theta \log \pi_{\theta}\left(\tau\right)  r(\tau)  ] \approx \frac{1}{N} \sum_{i=1}^{N}  \nabla_\theta \log \pi_{\theta}\left(\tau\right)  r(\tau)
$$


前面说到强化学习是试错学习，它会更新策略使得奖赏大的样本出现的概率越来越高，但由于训练过程中采样是随机的，可能会出现某个 $\tau$ 不被采样的情况。

- 如果 $r(\tau)$  只有正的情况，那么被采样到的 $\tau$ 的概率就会增加，这会导致没被采样到的 $\tau$ 的概率下降
- 因为采取的 $\tau$ 概率和为 1，可能存在归一化之后，好的 $\tau$ 的概率相对下降，坏的 $\tau$ 概率相对上升的情况。
- 如果最好的 $\tau$ 的  $r(\tau)=0$，那么采样到这些 $\tau$ 并不会更新策略提高其概率，因为此时计算得到的梯度为 0。
- 此外该目标函数基于 $r(\tau)$ 对策略的更新是很敏感的，如果采样到三个 $r(\tau)$，两正一负，更新的幅度会比较大（两个概率要增加，一个要减少），但如果给这三个 $r(\tau)$ 都加上一个常数使得其都为正，这时候其实更新的幅度会变小（三个的概率都要增加），虽然三个 $r(\tau)$ 的相对差不变，但是更新的幅度却不一样了，后者的方差明显比前者更大（概率密度曲线更平缓）。

<img width="480" src="/img/in-post/2020-04-18-强化学习思考（7）策略梯度.assets/v2-7e5e6fad9b4ea490110354f98d598ac6_1440w.jpg"/>

在有限采样的情况下，结果很依赖于初始策略的采样，导致了高方差问题，虽然增加样本能缓解这种高方差，但是增加样本也使得学习效率降低，下面介绍几种减少高方差的方法。



#### 修改累积奖赏 reward 计算方式

考虑到在时间 $t$ 采取的行动 action 与 $t$ 时期之前的奖赏 reward 无关，因此只需要将 $t$ 时刻开始到结束的 reward 进行加总。


$$
\nabla_{\theta} J_{PG}(\theta) \approx \frac{1}{N} \sum_{i=1}^{N}  \sum_{t=1}^{T} \left[\nabla_\theta \log \pi_{\theta}\left(a_{i,t} | s_{i,t}\right) \sum_{t'=t}^{T}   r\left({s}_{i,t'}, {a}_{i,t'}\right) \right]
$$


通过这样做数值求和上减少了，方差也倾向于随之减少。



#### 引入 baesline

我们希望好的轨迹的奖赏函数是正的，而差的轨迹是负的，然而之前也提到了它对值非常敏感，如果奖赏函数增加或者减少一个常数，结果就会很不同。因此需要引入一个基准线 baseline $b$。


$$
\nabla_{\theta} J_{PG}(\theta) = E_{\tau \sim \pi_\theta (\tau)} [\nabla_\theta \log \pi_{\theta}\left(\tau\right)  \left( r(\tau) - b \right) ] \approx \frac{1}{N} \sum_{i=1}^{N}  \nabla_\theta \log \pi_{\theta}\left(\tau\right)  \left( r(\tau) - b \right)
$$


可证明这里引入 baesline 不会改变期望的值，是无偏的


$$
E\left[\nabla_{\theta} \log \pi_{\theta}(\tau) b\right]=\int \pi_{\theta}(\tau) \nabla_{\theta} \log \pi_{\theta}(\tau) b d \tau=\int \nabla_{\theta} \pi_{\theta}(\tau) b d \tau=b \nabla_{\theta} \int \pi_{\theta}(\tau) d \tau=b \nabla_{\theta} 1=0
$$


其中 $b$ 可以自己设置，一种做法是使用 average reward：


$$
b = \frac{1}{N} \sum_{i=1}^N r(\tau_i)
$$



另一种做法是求解使得方差最小的 baseline：

因为 $Var[x] = E[x^2] - E[x]^2$，所以


$$
\begin{aligned}
Var &= E_{\tau \sim \pi_\theta (\tau)} [(\nabla_\theta \log \pi_{\theta}\left(\tau\right)  \left( r(\tau) - b \right) )^2] - E_{\tau \sim \pi_\theta (\tau)} [\nabla_\theta \log \pi_{\theta}\left(\tau\right)  \left( r(\tau) - b \right) ]^2
\\
&= E_{\tau \sim \pi_\theta (\tau)} [(\nabla_\theta \log \pi_{\theta}\left(\tau\right)  \left( r(\tau) - b \right) )^2] - E_{\tau \sim \pi_\theta (\tau)} [\nabla_\theta \log \pi_{\theta}\left(\tau\right)  r(\tau)]^2
\end{aligned}
$$


求导得


$$
\begin{aligned}
\frac{dVar}{db} &= \frac{d}{db}E_{\tau \sim \pi_\theta (\tau)} [(\nabla_\theta \log \pi_{\theta}\left(\tau\right)  \left( r(\tau) - b \right) )^2]
\\&= \frac{d}{db}(E[(\nabla_\theta \log \pi_{\theta}\left(\tau\right))^2  r(\tau)^2]
-2 E[(\nabla_\theta \log \pi_{\theta}\left(\tau\right))^2  r(\tau)b]+
b^2E[(\nabla_\theta \log \pi_{\theta}\left(\tau\right))^2])
\\&= -2 E[(\nabla_\theta \log \pi_{\theta}\left(\tau\right))^2  r(\tau)]+
2bE[(\nabla_\theta \log \pi_{\theta}\left(\tau\right))^2])
\\&=0
\end{aligned}
$$


解得


$$
b = \frac{E[(\nabla_\theta \log \pi_{\theta}\left(\tau\right))^2  r(\tau)]}{
E[(\nabla_\theta \log \pi_{\theta}\left(\tau\right))^2])}
$$


这其实是加权了梯度大小的期望奖赏，expected reward weighted by gradient magnitudes



## From on-policy to off-policy

### 基本术语

#### On-policy

学习的 agent 以及和环境进行互动的 agent 是**同一个agent**。



#### Off-policy

学习的 agent 以及和环境进行互动的 agent 是**不同的agent**。



**为什么要引入 Off-policy**

如果我们使用 $\pi_\theta$  来收集数据，那么参数 $\theta$ 被更新后，我们需要重新对训练数据进行采样，这样会造成巨大的时间消耗。而利用 $\pi_{\theta'}$ 来进行采样，将采集的样本拿来训练 $\theta$，$\theta'$是固定的，采集的样本可以被重复使用。



### Important sampling

我们可以使用 $q$ 分布来计算 $p$ 分布期望值。


$$
E_{x \sim p}[f(x)] =\int f(x) p(x) d x=\int f(x) \frac{p(x)}{q(x)} q(x) d x=E_{x \sim q}[f(x) \frac{p(x)}{q(x)}]
$$


需要注意的是，两个分布 $p$，$q$ 之间的差别不能太大，否则方差会出现较大的差别。


$$
\begin{aligned} \operatorname{Var}_{x \sim q}\left[f(x) \frac{p(x)}{q(x)}\right] &=E_{x \sim q}\left[\left(f(x) \frac{p(x)}{q(x)}\right)^{2}\right]-\left(E_{x \sim q}\left[f(x) \frac{p(x)}{q(x)}\right]\right)^{2} 

\\ &=\int \left[\left(f(x) \frac{p(x)}{q(x)}\right)^{2} q(x)\right] d x-\left(E_{x \sim p}[f(x)]\right)^{2}

\\ &=\int \left[\left(f(x) \frac{p(x)}{q(x)}\right)^{2} \frac{q(x)}{p(x)}p(x)\right] d x-\left(E_{x \sim p}[f(x)]\right)^{2}

\\ &=E_{x \sim p}\left[f(x)^{2} \frac{p(x)}{q(x)}\right]-\left(E_{x \sim p}[f(x)]\right)^{2} \end{aligned}
$$



可以发现两者得出的方差表达式后面一项相同，主要差别在于前面那一项，如果分布 $p$ 和 $q$ 之间差别太大，会导致第一项的值较大或较小，于是造成两者较大的差别。

如果分布 $p$ 和 $q$ 之间的差别过大，在训练的过程中就需要进行更多次数的采样，但这样会导致在采样上耗费较大的时间。



### 目标函数

使用 $\pi_{\theta'}$ 来采样多个回合的 $\tau$ 作为模型输入，然后对 Important sampling 后的 Expected Reward  进行求导。


$$
J(\theta)=  E_{\tau \sim \pi_{\theta^{\prime}}(\tau)}\left[\frac{\pi_{\theta}(\tau)}{\pi_{\theta^{\prime}}(\tau)} r(\tau) \right]
$$



其中


$$
\frac{\pi_{\theta}(\tau)}{\pi_{\theta^{\prime}}(\tau)} 

=\frac{p\left(s_{1}\right) \prod_{t=1}^{T} \pi_{\theta}\left(a_{t} | s_{t}\right) p\left(s_{t+1} | s_{t}, a_{t}\right)}{p\left(s_{1}\right) \prod_{t=1}^{T} \pi_{\theta'}\left(a_{t} | s_{t}\right) p\left(s_{t+1} | s_{t}, a_{t}\right)}

= \frac{\prod_{t=1}^{T} \pi_{\theta}\left(a_{t} | s_{t}\right) }{\prod_{t=1}^{T} \pi_{\theta'}\left(a_{t} | s_{t}\right) }
$$




### 梯度更新

对目标函数求梯度得


$$
\nabla_{\theta} J(\theta)=  E_{\tau \sim \pi_{\theta^{\prime}}(\tau)}\left[\frac{\nabla_{\theta}\pi_{\theta}(\tau)}{\pi_{\theta^{\prime}}(\tau)} r(\tau) \right] = E_{\tau \sim \pi_{\theta^{\prime}}(\tau)}\left[ \frac{\pi_{\theta}(\tau)}{\pi_{\theta^{\prime}}(\tau)}  \nabla_{\theta}\log \pi_{\theta}(\tau) r(\tau) \right]
\\
=E_{\tau \sim \pi_{\theta^{\prime}}(\tau)}\left[ \left( \frac{\prod_{t=1}^{T} \pi_{\theta}\left(a_{t} | s_{t}\right) }{\prod_{t=1}^{T} \pi_{\theta'}\left(a_{t} | s_{t}\right) } \right)  \left(\sum_{t=1}^{T} \nabla_\theta \log \pi_{\theta}\left(a_{i,t} | o_{i,t}\right) \right) \left(\sum_{t=1}^{T}   r\left({s}_{i,t}, {a}_{i,t}\right) \right) \right]
$$


如果 $\pi_{\theta}(\tau) = \pi_{\theta^{\prime}}(\tau)$，则与 on-policy 的策略梯度一致。

目前存在一个问题便是重要性采样中的累乘可能会导致该值特别的小，下面介绍两种方法：



#### 修改累积乘计算方式

同修改累积奖赏 reward 计算方式类似，在时间 $t$ 采取的行动 action 与 $t$ 时期之前的奖赏 reward 无关，在时间 $t$ 的重要性采样权重与 $t$ 时期之后的动作 action 无关。


$$
\require{cancel}
\nabla_{\theta} J(\theta)
=E_{\tau \sim \pi_{\theta^{\prime}}(\tau)}\left[ \left( \frac{\prod_{t=1}^{T} \pi_{\theta}\left(a_{t} | s_{t}\right) }{\prod_{t=1}^{T} \pi_{\theta'}\left(a_{t} | s_{t}\right) } \right)  \left(\sum_{t=1}^{T} \nabla_\theta \log \pi_{\theta}\left(a_{i,t} | s_{i,t}\right) \right) \left(\sum_{t=1}^{T}   r\left({s}_{i,t}, {a}_{i,t}\right) \right) \right]
\\
=E_{\tau \sim \pi_{\theta^{\prime}}(\tau)}\left[   \sum_{t=1}^{T} \left(\nabla_\theta \log \pi_{\theta}\left(a_{t} | s_{t}\right) \left( \prod_{t'=1}^{t}\frac{ \pi_{\theta}\left(a_{t'} | s_{t'}\right) }{ \pi_{\theta'}\left(a_{t'} | s_{t'}\right) } \right)\left(\sum_{t'=t}^{T}   r\left({s}_{t'}, {a}_{t'}\right) \cancel{\left( \prod_{t''=t}^{t'} \frac{ \pi_{\theta}\left(a_{t''} | s_{t''}\right) }{\pi_{\theta'}\left(a_{t''} | s_{t''}\right) } \right)} \right)\right)\right]
$$


忽略后一项可以得到策略迭代算法，这在后面会讲



#### A first-order approximation for IS

on-policy
$$
J(\theta) = \sum_{t=1}^{T} E_{\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right) \sim p_{\theta}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)}\left[r\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\right]
$$

$$
\nabla_{\theta} J_{PG}(\theta) \approx \frac{1}{N} \sum_{i=1}^{N}  \sum_{t=1}^{T} \left[\nabla_\theta \log \pi_{\theta}\left(a_{i,t} | s_{i,t}\right) \hat{Q}_{i,t} \right]
$$


off-policy

在两个层面上做重要性抽样


$$
J(\theta) = \sum_{t=1}^{T} E_{\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right) \sim p_{\theta}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)}\left[r\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\right]
\\ = \sum_{t=1}^{T} E_{\mathbf{s}_{t} \sim p_{\theta}\left(\mathbf{s}_{t}\right)}[E_{\mathbf{a}_{t} \sim \pi_{\theta}\left(\mathbf{a}_{t} | \mathbf{s}_{t} \right)}\left[r\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\right]]
\\ = \sum_{t=1}^{T} E_{\mathbf{s}_{t} \sim p_{\theta'}\left(\mathbf{s}_{t}\right)}\left[\frac{ p_{\theta}\left(s_{t}\right) }{p_{\theta'}\left(s_{t}\right) }E_{\mathbf{a}_{t} \sim \pi_{\theta'}\left(\mathbf{a}_{t} | \mathbf{s}_{t} \right)}\left[\frac{ \pi_{\theta}\left(a_{t} | s_{t}\right) }{\pi_{\theta'}\left(a_{t} | s_{t}\right) }r\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\right]\right]
$$


展开结果如下


$$
\require{cancel}
\nabla_{\theta} J_{PG}(\theta) \approx \frac{1}{N} \sum_{i=1}^{N}  \sum_{t=1}^{T} \left[\cancel{\frac{ p_{\theta}\left(s_{i,t}\right) }{p_{\theta'}\left(s_{i,t}\right) }}\frac{ \pi_{\theta}\left(a_{i,t} | s_{i,t}\right) }{\pi_{\theta'}\left(a_{i,t} | s_{i,t}\right) } \nabla_\theta \log \pi_{\theta}\left(a_{i,t} | s_{i,t}\right)  \hat{Q}_{i,t} \right] \quad\fbox{1}
$$


$\fbox{1}$  虽然这样会引入一定的偏差，当两个策略非常接近的情况下，某一个状态 state 出现的概率几乎没有差别，因此可以将这一项近似地消掉。



## Advantages of Policy-Based RL

Advantages:

- Better convergence properties
- Effective in high-dimensional or continuous action spaces
- Can learn stochastic policies
  - in some case we don't need a deterministic policy, maybe a uniform random policy is optimal (i.e. Nash equilibrium)
  - Value-based RL learns a near-deterministic policy (e.g. greedy or ε-greedy)

Disadvantages: 

- Typically converge to a local rather than global optimum
- Evaluating a policy is typically inefficient and high variance



## Policy Gradient Theorem

Policy Objective Functions

- In episodic environments we can use the start value


$$
J_{1}(\theta)=V^{\pi_{\theta}}\left(s_{1}\right)=\mathbb{E}_{\pi_{\theta}}\left[v_{1}\right]
$$


- In continuing environments we can use the average value


$$
J_{a v V}(\theta)=\sum_{s} d^{\pi_{\theta}}(s) V^{\pi_{\theta}}(s)
$$


- Or the average reward per time-step


$$
J_{a v R}(\theta)=\sum_{s} d^{\pi_{\theta}}(s) \sum_{a} \pi_{\theta}(s, a) \mathcal{R}_{s}^{a}
$$


where $d^\pi(s)$  is stationary distribution of Markov chain for $\pi_\theta$



>Theorem
>
>For any differentiable policy $\pi_{\theta}(s, a)$,
>
>for any of the policy objective functions $J = J_1$, $J_{avR}$, or $\frac{1}{1-\gamma} J_{avV}$,
>
>the policy gradient is
>
>
>$$
>\nabla_{\theta} J(\theta) = E_{\pi_\theta} [\nabla_\theta \log \pi_{\theta}\left(s,a\right) Q^{\pi_\theta}(s,a) ]
>$$
>



## 参考资料及致谢

所有参考资料在前言中已列出，再次向强化学习的大佬前辈们表达衷心的感谢！

