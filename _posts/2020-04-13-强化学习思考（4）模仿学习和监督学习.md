---
layout:     post
title:      "强化学习思考（4）模仿学习和监督学习"
subtitle:    "模仿学习和监督学习"
date:       2020-04-13 10:00:00
author:     Shunyu
header-img: img/post-bg-2015.jpg
header-mask: 0.1
catalog: true
mathjax: true
tags:
    - 强化学习
    - 强化学习思考
---



关于模仿学习和监督学习的注意事项。



## 目录

- [强化学习思考（1）前言](https://liushunyu.github.io/2020/04/12/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E6%80%9D%E8%80%83-1-%E5%89%8D%E8%A8%80/)
- [强化学习思考（2）强化学习简介](https://liushunyu.github.io/2020/04/12/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E6%80%9D%E8%80%83-2-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/)
- [强化学习思考（3）马尔可夫决策过程](https://liushunyu.github.io/2020/04/12/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E6%80%9D%E8%80%83-3-%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B/)
- [强化学习思考（4）模仿学习和监督学习](https://liushunyu.github.io/2020/04/13/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E6%80%9D%E8%80%83-4-%E6%A8%A1%E4%BB%BF%E5%AD%A6%E4%B9%A0%E5%92%8C%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/)



## DAgger: Dataset Aggregation

我们的目标是使得 $P_{data}(o_t) = P_{\pi_\theta}(o_t)$，可以从 $P_{\pi_\theta}(o_t)$ 中收集训练数据取代 $P_{data}(o_t)$：

1、train $\pi_\theta(a_t \mid o_t)$ from human data $\cal{D} = \{o_1, a_1,...,o_N, a_N\}$

2、run $\pi_\theta(a_t \mid o_t)$ to get dataset $\cal{D}_\pi = \{ o_1, ..., o_M \}$

3、Ask human to label $\cal{D}_\pi$ with actions $a_t$

4、Aggregate: $\cal{D} \leftarrow \cal{D} \cup \cal{D}_\pi$

DAgger addresses the problem of distributional “drift”



## How bad is the policy?

定义损失函数如下：


$$
c(\mathbf{s}, \mathbf{a})=\left\{\begin{array}{l}
0 \text { if } \mathbf{a}=\pi^{\star}(\mathbf{s}) \\
1 \text { otherwise }
\end{array}\right.
$$



### 情况 1



**Assume: **


$$
\pi_{\theta}\left(\mathbf{a} \neq \pi^{\star}(\mathbf{s}) | \mathbf{s}\right) \leq \epsilon
\text { for all } \mathbf{s} \in \mathcal{D}_{\text {train }}
$$


**Result:**


$$
\begin{aligned}
&E\left[\sum_{t} c\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\right] \leq \underbrace{\epsilon T+(1-\epsilon)(\epsilon(T-1)+(1-\epsilon)(\ldots))}_{T \text { terms }*\text{ each } O(\epsilon T) \;= \; O(\epsilon T^2)}
\end{aligned}
$$



### 情况 2



**Assume: **


$$
\pi_{\theta}\left(\mathbf{a} \neq \pi^{\star}(\mathbf{s}) | \mathbf{s}\right) \leq \epsilon\text { for all } \mathbf{s} \sim p_{\text {train }}(\mathbf{s})
$$


**With DAgger:**


$$
p_{\text {train }}(\mathbf{s}) \to p_{\theta}(\mathbf{s})
$$


**Result:**


$$
\begin{aligned}
E\left[\sum_{t} c\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\right] \leq \epsilon T
\end{aligned}
$$



### 情况 3



**Assume: **


$$
\pi_{\theta}\left(\mathbf{a} \neq \pi^{\star}(\mathbf{s}) | \mathbf{s}\right) \leq \epsilon\text { for all } \mathbf{s} \sim p_{\text {train }}(\mathbf{s})
$$


**If:**


$$
p_{\text {train }}(\mathbf{s}) \neq p_{\theta}(\mathbf{s})
$$


**Result:**

- 第一步：



$$
p_{\theta}\left(\mathbf{s}_{t}\right)=\underbrace{(1-\epsilon)^{t}}_{probability we made no mistakes} p_{\text {train }}\left(\mathbf{s}_{t}\right)+\left(1-(1-\epsilon)^{t}\right) \underbrace{p_{\text {mistake }}\left(\mathbf{s}_{t}\right)}_{some other distribution}
$$



- 第二步：


$$
\begin{aligned}
&\left|p_{\theta}\left(\mathbf{s}_{t}\right)-p_{\text {train }}\left(\mathbf{s}_{t}\right)\right|

\\=&\left(1-(1-\epsilon)^{t}\right)\left|p_{\text {mistake }}\left(\mathbf{s}_{t}\right)-p_{\text {train }}\left(\mathbf{s}_{t}\right)\right| 

\\ \leq & 2\left(1-(1-\epsilon)^{t}\right)
\\ \leq & 2 \epsilon t
 
 \end{aligned}
$$



其中最后一步使用定理：$(1-\epsilon)^{t} \geq 1-\epsilon t$ for $\epsilon \in[0,1]$ 

- 第三步：



$$
\begin{aligned}
&\sum_{t} E_{p_{\theta}\left(\mathbf{s}_{t}\right)}\left[c_{t}\right]

\\=&\sum_{t} \sum_{\mathbf{s}_{t}} p_{\theta}\left(\mathbf{s}_{t}\right) c_{t}\left(\mathbf{s}_{t}\right) \\ \leq & \sum_{t} \sum_{\mathbf{s}_{t}} p_{\text {train }}\left(\mathbf{s}_{t}\right) c_{t}\left(\mathbf{s}_{t}\right)+\left|p_{\theta}\left(\mathbf{s}_{t}\right)-p_{\text {train }}\left(\mathbf{s}_{t}\right)\right| c_{\max } \\  \leq & \underbrace{\sum_{t} \epsilon+2 \epsilon t}_{O(\epsilon T^2)}
\end{aligned}
$$



For more analysis, see Ross et al. “A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning”



## Why might we fail to fit the expert? 

1、Non-Markovian behavior 

- use the whole history: RNN



$$
\pi_\theta(a_t|o_t) \to \pi_\theta(a_t|o_1,...,o_t)
$$



2、Multimodal behavior

- Output mixture of Gaussians
- Latent variable models
- Autoregressive discretization



## Goal-conditioned behavioral cloning

将 policy function 修改为目标相关的，这样便可以学习不同目标下的 policy function



$$
\pi_\theta(a|s) \to \pi_\theta(a|s,g)
$$



where $g$ is the goal state.



## 参考资料及致谢

所有参考资料在前言中已列出，再次向强化学习的大佬前辈们表达衷心的感谢！
