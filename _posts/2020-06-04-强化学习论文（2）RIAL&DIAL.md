---
layout:     post
title:      "强化学习论文（2）RIAL&DIAL"
subtitle:   "Learning to Communicate with Deep Multi-Agent Reinforcement Learning"
date:       2020-06-04 10:00:00
author:     Shunyu
header-img: img/post-bg-2015.jpg
header-mask: 0.1
catalog: true
mathjax: true
tags:
    - 强化学习
    - 强化学习论文
---



**标签：**

RIAL; DIAL; value-based; on-policy; model-free; communication; discrete communication channel; discrete action space; continuous state space; cooperative task; centralized training with decentralized execution; multi-agent;



[论文链接](https://arxiv.org/abs/1605.06676)



## 创新点及贡献

1、首个通过深度学习学习多智能体之间的通信协议的方法，基于 DRQN 提出了一种集中训练分散执行的多智能体学习通信协议的框架。

2、集中训练分散执行的框架在于训练时所有的智能体参数共享同一个网络，在执行时每个智能体复制一份网络并拥有自己独立的隐藏状态，其中根据通信信息是否可以反向传播拥有反馈机制分为 RIAL 方法和 DIAL 方法。



## 研究痛点

1、目前大多数研究都是预先定义好通信协议，而未有研究可以自动化地学习智能体之间的通信协议。



2、论文关注点在于集中训练分散执行的框架设计

- 因为智能体在训练时的通信通常可以不受限制，而在执行时往往会受到带宽限制，比如机器人仿真实验。
- 智能体需要协调消息的发送和解析之间的关系，这加剧了探索协议空间的困难，比如智能体 A 发送一条有用的消息给另一个智能体 B，但是仅当智能体 B 成功解析该条消息并执行正确的动作时智能体 A 才会获得正的 reward，因此正的 reward 会变得稀疏，使得随机探索成功的难度加大。



## 算法流程

框架图如下

<img width="90%" src="/img/in-post/2020-06-04-强化学习论文（2）RIAL&DIAL.assets/image-20200604231902877.png"/>



### RIAL (Reinforced Inter-Agent Learning)

1、基于 DRQN 设计，注意不使用经验回放池。



2、为了避免输出维度过大，将 Q-Net 网络分为输出通信动作的 $Q_m$ 与输出环境动作的 $Q_u$，输出维度便从 $\mid U\mid \mid M\mid $ 降到了 $U\mid +\mid \mid M\mid $，具体 $Q_u$ 网络输入定义如下，$Q_m$ 与之相同。

<img height="50%" src="/img/in-post/2020-06-04-强化学习论文（2）RIAL&DIAL.assets/image-20200604230641715.png"/>

其中 $o_t^a$ 是当前智能体的观察，$m_{t-1}^{a'}$ 是另一个智能体发送的通信动作，$h_{t-1}^a$ 是当前智能体的的隐藏状态， $u_{t-1}^a$ 是当前智能体上一个时刻的环境动作，$m_{t-1}^{a}$ 是当前智能体上一个时刻发出的通信动作，$a$ 是当前智能体的编号，$u_{t-1}^a$ 是当前智能体的环境动作。



3、Q-Net 网络的 Q-value 输出给 Action Select 通过 $\epsilon$-贪心来选择动作，注意这里的通信动作和环境动作都是 one-hot 编码。



4、集中训练分散执行的框架在于训练时所有的智能体参数共享同一个网络，在执行时每个智能体复制一份网络并拥有自己独立的隐藏状态。



### DIAL (Differentiable Inter-Agent Learning)

1、基于 DRQN 设计，注意不使用经验回放池。



2、与 RIAL 不同，DIAL 将 Q-Net 修改为 C-Net，其环境动作的 Q-value 输出给 Action Select，其通信动作的网络输出真实值绕过 Action Select 通过 discretise/regularise unit (DRU) 输入给别的智能体，这样便可以通过梯度的反向传播实现反馈机制，



3、集中训练分散执行的框架同样通过参数共享来实现，此外在训练时 $DRU(m^a_t) = Logistic({\mathcal{N}}(m^a_t,\sigma))$，执行时 $DRU(m^a_t) = {\mathbb{1}}_{m^a_t > 0}$，注意这里的通信动作是二进制编码，而 RIAL 的通信动作是 one-hot 编码。



#### 算法伪代码

DIAL 算法伪代码如下

<img width="90%" src="/img/in-post/2020-06-04-强化学习论文（2）RIAL&DIAL.assets/image-20200605101711825.png"/>



#### 模型架构

DIAL 模型架构如下

<img width="50%" src="/img/in-post/2020-06-04-强化学习论文（2）RIAL&DIAL.assets/image-20200605102349291.png"/>



## 实验

1、实验环境采用了 Switch Riddle 和 MNIST Games

- Switch Riddle: 可参考[100个囚犯和灯泡的那些事儿](http://www.matrix67.com/blog/archives/3618)

<img width="50%" src="/img/in-post/2020-06-04-强化学习论文（2）RIAL&DIAL.assets/image-20200605102623462.png"/>



- MNIST Games:
  - Colour-Digit MNIST：每次传递 1-bit 信息，共两次，辨别两种颜色和数字奇偶
  - Multi-Step MNIST：每次传递 1-bit 信息，共五次，辨别十种数字类别

<img width="70%" src="/img/in-post/2020-06-04-强化学习论文（2）RIAL&DIAL.assets/image-20200605103038787.png"/>



2、值得注意的是：

- Switch Riddle 仅在 $n=3$ 和 $n=4$ 上进行了实验，个人感觉智能体太少
- Multi-Step MNIST 在可视化通信动作之后发现智能体成功学会为十种数字类别进行了二进制编码



## 其他补充

1、论文还分析了在 DIAL 中的 DRU 通信信道中加入噪声 $\sigma$ 对成功训练的有效性。



## 参考资料及致谢

所有参考资料在《强化学习思考（1）前言》中已列出，再次向强化学习的大佬前辈们表达衷心的感谢！

