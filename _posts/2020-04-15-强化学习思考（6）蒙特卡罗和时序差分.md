---
layout:     post
title:      "强化学习思考（6）蒙特卡罗和时序差分"
subtitle:    "蒙特卡罗和时序差分"
date:       2020-04-15 11:00:00
author:     Shunyu
header-img: img/post-bg-2015.jpg
header-mask: 0.1
catalog: true
mathjax: true
tags:
    - 强化学习
    - 强化学习思考
---



关于蒙特卡罗和时序差分的注意事项。



## 目录

- [强化学习思考（1）前言](https://liushunyu.github.io/2020/04/12/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E6%80%9D%E8%80%83-1-%E5%89%8D%E8%A8%80/)
- [强化学习思考（2）强化学习简介](https://liushunyu.github.io/2020/04/12/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E6%80%9D%E8%80%83-2-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/)
- [强化学习思考（3）马尔可夫决策过程](https://liushunyu.github.io/2020/04/12/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E6%80%9D%E8%80%83-3-%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B/)
- [强化学习思考（4）模仿学习和监督学习](https://liushunyu.github.io/2020/04/13/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E6%80%9D%E8%80%83-4-%E6%A8%A1%E4%BB%BF%E5%AD%A6%E4%B9%A0%E5%92%8C%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/)
- [强化学习思考（5）动态规划](https://liushunyu.github.io/2020/04/15/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E6%80%9D%E8%80%83-5-%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/)
- [强化学习思考（6）蒙特卡罗和时序差分](https://liushunyu.github.io/2020/04/15/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E6%80%9D%E8%80%83-6-%E8%92%99%E7%89%B9%E5%8D%A1%E7%BD%97%E5%92%8C%E6%97%B6%E5%BA%8F%E5%B7%AE%E5%88%86/)
- [强化学习思考（7）策略梯度](https://liushunyu.github.io/2020/04/18/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E6%80%9D%E8%80%83-7-%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6/)
- [强化学习思考（8）Actor-Critic 方法](https://liushunyu.github.io/2020/04/21/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E6%80%9D%E8%80%83-8-Actor-Critic-%E6%96%B9%E6%B3%95/)
- [强化学习思考（9）值函数方法](https://liushunyu.github.io/2020/04/22/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E6%80%9D%E8%80%83-9-%E5%80%BC%E5%87%BD%E6%95%B0%E6%96%B9%E6%B3%95/)
- [强化学习思考（10）Deep Q Network](https://liushunyu.github.io/2020/04/22/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E6%80%9D%E8%80%83-10-Deep-Q-Network/)
- [强化学习思考（11）Advanced Policy Gradient](https://liushunyu.github.io/2020/04/23/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E6%80%9D%E8%80%83-11-Advanced-Policy-Gradient/)



## MC and TD

### Basic definition

#### Monte-Carlo Learning

- MC methods learn directly from episodes of experience
- MC is model-free: no knowledge of MDP transitions / rewards
- MC learns from complete episodes: no bootstrapping
- MC uses the simplest possible idea: value = mean return
- Caveat: can only apply MC to episodic MDPs
  - All episodes must terminate



#### Temporal-Difference Learning

- TD methods learn directly from episodes of experience
- TD is model-free: no knowledge of MDP transitions / rewards
- TD learns from incomplete episodes, by bootstrapping
- TD updates a guess towards a guess



#### Advantages and Disadvantages of MC vs. TD

- TD can learn before knowing the final outcome
  - TD can learn online after every step
  - MC must wait until end of episode before return is known
- TD can learn without the final outcome
  - TD can learn from incomplete sequences
  - MC can only learn from complete sequences
  - TD works in continuing (non-terminating) environments
  - MC only works for episodic (terminating) environments
- initial value
  - MC is not very sensitive to initial value
  - TD is more sensitive to initial value than MC



### Bias / Variance Trade-Off

- Return $G_t = R_{t+1} + \gamma R_{t+2} + ... + \gamma^{T−1} R_T$ is unbiased estimate of $v_\pi(S_t)$

- True TD target $R_{t+1} + \gamma v_\pi(S_{t+1})$ is unbiased estimate of $v_\pi(S_t)$

- TD target $R_{t+1} + \gamma v_\pi(S_{t+1})$ is biased estimate of $v_\pi(S_t)$
- TD target is much lower variance than the return:
  - Return depends on many random actions, transitions, rewards
  - TD target depends on one random action, transition, reward



#### Advantages and Disadvantages of MC vs. TD

- MC has high variance, zero bias
  - Good convergence properties
  - (even with function approximation)
  - Not very sensitive to initial value
  - Very simple to understand and use

- TD has low variance, some bias
  - Usually more efficient than MC
  - TD(0) converges to $v_\pi(S_t)$
  - (but not always with function approximation)
  - More sensitive to initial value



### Batch MC and TD

MC and TD converge: $V(s) → v_\pi(s)$ as experience $\to \infty$, but the batch solution for finite experience may not equal

- MC converges to solution with minimum mean-squared error
  - Best fit to the observed returns

  

$$
\sum_{k=1}^{K} \sum_{t=1}^{T_{k}}\left(G_{t}^{k}-V\left(s_{t}^{k}\right)\right)^{2}
$$



- TD(0) converges to solution of max likelihood Markov model
  - Solution to the MDP that best fits the data



$$
\begin{aligned}
\hat{\mathcal{P}}_{s, s^{\prime}}^{a} &=\frac{1}{N(s, a)} \sum_{k=1}^{K} \sum_{t=1}^{T_{k}} \mathbf{1}\left(s_{t}^{k}, a_{t}^{k}, s_{t+1}^{k}=s, a, s^{\prime}\right) \\
\hat{\mathcal{R}}_{s}^{a} &=\frac{1}{N(s, a)} \sum_{k=1}^{K} \sum_{t=1}^{T_{k}} \mathbf{1}\left(s_{t}^{k}, a_{t}^{k}=s, a\right) r_{t}^{k}
\end{aligned}
$$



#### Advantages and Disadvantages of MC vs. TD

- TD exploits Markov property
  - Usually more efficient in Markov environments

- MC does not exploit Markov property
  - Usually more effective in non-Markov environments





## Convergence

### Greedy in the Limit with Infinite Exploration (GLIE)

<img width="480" src="/img/in-post/2020-04-15-强化学习思考（6）蒙特卡罗和时序差分.assets/image-20200415182827589.png"/>



#### GLIE Monte-Carlo control

>Theorem
>
>GLIE Monte-Carlo control converges to the optimal action-value function, $Q(s, a) \to q_∗(s, a)$



#### Sarsa

>Theorem
>
>Sarsa converges to the optimal action-value function, $Q(s, a) \to q_∗(s, a)$, under the following conditions:
>
>- GLIE sequence of policies $\pi_t(a \mid s)$
>- Robbins-Monro sequence of step-sizes $\alpha_t$
>
>
>$$
>\sum_{t=1}^\infty \alpha_t = \infty
>\\
>\sum_{t=1}^\infty \alpha_t^2 < \infty
>$$
> 



#### Q-Learning

>Theorem
>
>Q-learning control converges to the optimal action-value function, $Q(s, a) \to q_∗(s, a)$





## Unified View of Reinforcement Learning

<img width="480" src="/img/in-post/2020-04-15-强化学习思考（6）蒙特卡罗和时序差分.assets/image-20200415175417346.png"/>



### Bootstrapping and Sampling

- Bootstrapping: update involves an estimate MC does not bootstrap
  - DP bootstraps
  - TD bootstraps

- Sampling: update samples an expectation
  - MC samples
  - DP does not sample
  - TD samples



### Relationship Between DP and TD

<img width="580" src="/img/in-post/2020-04-15-强化学习思考（6）蒙特卡罗和时序差分.assets/image-20200415175610452.png"/>

<img width="580" src="/img/in-post/2020-04-15-强化学习思考（6）蒙特卡罗和时序差分.assets/image-20200415175652645.png"/>

where $x \;{\xleftarrow{a}}\; y \equiv x \leftarrow x+\alpha(y−x)$



## 参考资料及致谢

所有参考资料在前言中已列出，再次向强化学习的大佬前辈们表达衷心的感谢！
