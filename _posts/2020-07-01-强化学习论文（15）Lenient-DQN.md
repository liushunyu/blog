---

layout:     post
title:      "强化学习论文（15）Lenient-DQN"
subtitle:   "Lenient Multi-Agent Deep Reinforcement Learning"
date:       2020-07-01 09:00:00
author:     Shunyu
header-img: img/post-bg-2015.jpg
header-mask: 0.1
catalog: true
mathjax: true
tags:
    - 强化学习
    - 强化学习论文
---



**标签：**

Lenient-DQN; value-based; off-policy; model-free; discrete action space; continuous state space; cooperative task; decentralized approach; multi-agent;



[论文链接](https://arxiv.org/abs/1707.04402)



## 创新点及贡献

1、论文提出了 Lenient-DQN 方法，将 leniency 方法应用到具有高维状态空间的 MA-DRL 问题中，通过将状态动作对映射到逐渐衰减的温度值，这些温度值控制对从经验池采样的负学习更新应用 leniency 处理的程度。



2、leniency 方法的目的是防止 relative overgeneralization，即当各 agent 的探索策略对其他 agent 的学习更新产生相互影响而产生噪声时，agent倾 向于一个鲁棒但次优的联合策略。



3、引入了两种 leniency 机制：

- retroactive temperature decay schedule (TDS) 防止过早冷却温度
- $\bar{T}(s)$-Greedy 探索策略，选择最佳动作的概率基于当前状态的平均温度



## 研究痛点

1、MARL 问题因为 moving target problem 具有较大的非平稳性，即当前智能体的 reward 会因为别的智能体的动作而改变。



2、Hysteretic-DQNs 无法处理具有随机 reward 的合作场景，并陷入局部最优解。



3、经验池中的经验会超时失效而无法再使用。



4、leniency 方法之前都是应用在表格式方法中，没有处理高维连续状态空间。



## 算法流程

算法框架如下

<img width="100%" src="/img/in-post/2020-07-01-强化学习论文（15）Lenient-DQN.assets/image-20200701163722599.png"/>



### 主要思路

1、对高维连续状态进行编码为离散值

<img width="80%" src="/img/in-post/2020-07-01-强化学习论文（15）Lenient-DQN.assets/image-20200701162247000.png"/>



2、计算当前的温度值，分为两种方法，分别是 Average Temperature Folding (ATF) 和 Retroactive Temperature Decay Schedule (TDS)，都是为了解决初期的状态被过多访问导致温度下降过快的问题。针对每一个状态动作对都有一个温度，且需要在一开始定义好初始最大温度。

- Average Temperature Folding (ATF)

<img width="80%" src="/img/in-post/2020-07-01-强化学习论文（15）Lenient-DQN.assets/image-20200701163453226.png"/>

- Retroactive Temperature Decay Schedule (TDS)：其中 $\beta_n$ 在 episode 的初期是比较大的，在接近终止状态时是比较小的，因此初始状态的降温速度慢于接近终止状态的。 

<img width="80%" src="/img/in-post/2020-07-01-强化学习论文（15）Lenient-DQN.assets/image-20200701163551052.png"/>

<img width="80%" src="/img/in-post/2020-07-01-强化学习论文（15）Lenient-DQN.assets/image-20200701163602493.png"/>



3、计算 leniency 值，并将其随着经验同时存储到经验池中。

<img width="30%" src="/img/in-post/2020-07-01-强化学习论文（15）Lenient-DQN.assets/image-20200701163636424.png"/>



4、从经验池中采样进行更新操作

- 随着时间 $t$ 的增大，温度 $T$ 慢慢下降，leniency 值也随之下降，此时负误差被学习更新的概率则上升。

<img width="80%" src="/img/in-post/2020-07-01-强化学习论文（15）Lenient-DQN.assets/image-20200701163843699.png"/>



5、采用了平均温度作为探索的概率，随着时间 $t$ 的增大，平均温度 $\bar{T}(s)$ 慢慢下降，探索的概率也慢慢下降

<img width="80%" src="/img/in-post/2020-07-01-强化学习论文（15）Lenient-DQN.assets/image-20200701164432476.png"/>



6、对 Hysteretic-DQN 进行改进提出了 Scheduled-HDQN，希望初始状态的更新速率慢于接近终止状态的更新速率。

<img width="80%" src="/img/in-post/2020-07-01-强化学习论文（15）Lenient-DQN.assets/image-20200701164801397.png"/>



## 实验

1、在三个双智能体的协作环境中进行实验，其中 Stochastic Reward 环境的奖赏是随机分布的。

<img width="100%" src="/img/in-post/2020-07-01-强化学习论文（15）Lenient-DQN.assets/image-20200701162637909.png"/>



## 其他补充

1、从实验结果上看，在 Original 和 Narrow-Passgae 环境中感觉没有比 HDQN 效果好多少，而且训练时间还要求更久，在 Stochastic Reward 中效果较佳，但是只在两个智能体的环境上测试有点一般。

2、本文超参数感觉挺多的，都挺先验的东西，感觉使用起来不实际。



## 参考资料及致谢

所有参考资料在《强化学习思考（1）前言》中已列出，再次向强化学习的大佬前辈们表达衷心的感谢！

