---

layout:     post
title:      "强化学习论文（10）QTRAN"
subtitle:   "QTRAN: Learning to Factorize with Transformation for Cooperative Multi-Agent Reinforcement learning"
date:       2020-06-19 09:00:00
author:     Shunyu
header-img: img/post-bg-2015.jpg
header-mask: 0.1
catalog: true
mathjax: true
tags:
    - 强化学习
    - 强化学习论文
---



**标签：**

QTRAN; value-based; off-policy; model-free; discrete action space; continuous state space; cooperative task; credit assignment; centralized training with decentralized execution; value function factorization; multi-agent;



[论文链接](https://arxiv.org/abs/1905.05408)



## 创新点及贡献

1、在 VDN 和 QMIX 算法的基础上，QTRAN 提出了一种更加泛化的值分解方法，从而成功分解任何可分解的任务。



## 研究痛点

1、VDN 的加法性值分解和 QMIX 的单调性值分解方法都因为其结构化的约束无法有效分解一些可以分解的任务。



## 算法流程

### 前提假设

1、IGM Condition and Factorizable Task

- 如果一个任务的联合动作价值函数对应的最优动作和独立动作价值函数对应的最优动作一致，即满足 IGM 条件，则称该任务为可分解的任务。

<img width="60%" src="/img/in-post/2020-06-19-强化学习论文（10）QTRAN.assets/image-20200622104350499.png"/>



2、VDN 和 QMIX 满足 IGM 的充分条件

- VDN 和 QMIX 可以很好地分解满足其条件的联合动作价值函数。 但是存在一些任务的联合动作价值函数不满足其条件，因此其具有局限性。

<img width="60%" src="/img/in-post/2020-06-19-强化学习论文（10）QTRAN.assets/image-20200622104631062.png"/>



### 主要思路

1、QTRAN 提出的满足 IGM 的充分条件

<img width="60%" src="/img/in-post/2020-06-19-强化学习论文（10）QTRAN.assets/image-20200622105438356.png"/>



2、定义一个 transformed joint-action value function，用于逼近 factorizable joint action-value function。

- $Q_{jt}'$ 的最优动作和 $Q_{jt}$ 的最优动作是一致的。
- 因此使用 $Q_{jt}'$ 来逼近 $Q_{jt}$，而函数 $V_{jt}$ 可以认为是对它们之间的差异的校正，这种差异是由智能体的部分可观察性引起的，如果具有完全的可观察性，则可以将 $V_{jt}$ 定义为零。

<img width="60%" src="/img/in-post/2020-06-19-强化学习论文（10）QTRAN.assets/image-20200622110714173.png"/>



3、Overall framework：主体模型分为三个部分

- 采用集中训练分散执行的框架，在执行时每个智能体都利用自己独立的动作价值函数进行动作选择。

<img width="60%" src="/img/in-post/2020-06-19-强化学习论文（10）QTRAN.assets/image-20200622112710939.png"/>



- 模型框架图如下

<img width="100%" src="/img/in-post/2020-06-19-强化学习论文（10）QTRAN.assets/image-20200622112956856.png"/>



4、QTRAN-base 损失函数定义如下，由其充分条件中的约束得到。

<img width="50%" src="/img/in-post/2020-06-19-强化学习论文（10）QTRAN.assets/image-20200622112412553.png"/>

<img width="60%" src="/img/in-post/2020-06-19-强化学习论文（10）QTRAN.assets/image-20200622112428552.png"/>



5、在 QTRAN-base 的充分条件中的 4(b) 约束过于松弛，因为训练数据中大部分数据都是 $u \neq \bar{u}$ 的，满足第二个约束条件，使得算法很难成功拟合联合动作价值函数，损害了训练过程中的稳定性及收敛速度。因此 QTRAN-alt 提出了一个更强的约束条件替换掉上述的 4(b) 约束，这个约束更多的聚焦于 $u \neq \bar{u}$ 的数据，并证明了最终得到的结果是一致的。

- the newly devised condition compels $Q_{jt}'$ to track $Q_{jt}$

<img width="60%" src="/img/in-post/2020-06-19-强化学习论文（10）QTRAN.assets/image-20200622114347843.png"/>



6、QTRAN-alt 将第三个网络定义为 Counterfactual joint networks，第三个损失函数修改如下，另外两个与 QTRAN-base 一致

<img width="60%" src="/img/in-post/2020-06-19-强化学习论文（10）QTRAN.assets/image-20200622114710157.png"/>



## 实验

1、首先在简单实验 Single-state Matrix Game 上证明 QTRAN 效果优于 VDN 和 QMIX。

2、接着在 Multi-domain Gaussian Squeeze 和 Modified predator-prey 上进行实验。



## 其他补充

1、感觉该算法的复杂度应该是挺高的，所以实验的环境都是比较小型的，而且无法撑起较多智能体的环境。

2、该论文认为其可以分解任意可分解的任务，那么就还遗留无法分解的协作任务的问题应该怎么办。



## 参考资料及致谢

所有参考资料在《强化学习思考（1）前言》中已列出，再次向强化学习的大佬前辈们表达衷心的感谢！

