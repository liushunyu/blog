---

layout:     post
title:      "强化学习论文（11）ATOC"
subtitle:   "Learning Attentional Communication for Multi-Agent Cooperation"
date:       2020-06-23 09:00:00
author:     Shunyu
header-img: img/post-bg-2015.jpg
header-mask: 0.1
catalog: true
mathjax: true
tags:
    - 强化学习
    - 强化学习论文
---



**标签：**

ATOC; actor-critic; off-policy; model-free; communication; continuous communication channel; continuous action space; continuous state space; cooperative task; centralized training with decentralized execution; multi-agent;



[论文链接](https://arxiv.org/abs/1805.07733v1)



## 创新点及贡献

1、ATOC 在 DDPG 算法的基础上加入了 Attentional Communication 的机制，使得智能体可以自己决定是否与其他智能体进行通信，并且每个智能体只需要与部分智能体进行通信。

2、ATOC 实现了大规模智能体之间的通信协作。



## 研究痛点

1、对于 DIAL、CommNet、BiCNet 方法来说，所有智能体之间需要同时进行通信，当智能体数目较大时，智能体便无法区分通信信息中的有用信息，从而导致无法进行有效协作。

2、在大规模智能体通信的现实问题中，需要较大的带宽，而且可能有较长时间的延时和高计算复杂度。

3、预定义的通信协议会限制特定智能体之间的通信，从而限制潜在的合作。



## 算法流程

ATOC 框架图

<img width="60%" src="/img/in-post/2020-06-23-强化学习论文（11）ATOC.assets/image-20200623155715381.png"/>



### 主要思路

1、基于 DDPG 算法进行设计，所有的智能体共享同一个 actor 网络和 critic 网络，其中通信在 actor 网络的中间层实现。



2、每个智能体将自己的局部观察输入到 ActorNet (1) 中，输出隐藏层编码向量 $h_t^i$。

<img width="30%" src="/img/in-post/2020-06-23-强化学习论文（11）ATOC.assets/image-20200623161725655.png"/>



3、然后将该隐藏层编码向量 $h_t^i$ 输入给 Attention Unit，Attention Unit 是个二分类器，决定该智能体是否需要进行通信，如果需要则将其称为 initiator，可以从其视野范围内选择其他的智能体作为 collaborators，形成一个 communication group。

- 这里有个参数 $T$，代表接下来的 $T$ 的时刻保持该 communication group 不变。
- Attention Unit 可以通过 MLP 实现也可以通过 RNN 实现。



4、其视野范围内选择其他的智能体作为 collaborators 将基于以下规则：

- 假设具有一个固定的通信带宽，所以每个 initiator 智能体只能选择 $m$ 个 collaborators。
- 首先选择没有被选择过的智能体，然后选择被其他 initiator 选择了的智能体，最后选择其他 initiators。



5、然后采用了双向 LSTM 网络来构建通信信道 communication channel（与 BiCNet 类似），该 LSTM 网络将 initiator 及其 collaborators 的隐藏层编码向量 $h_t^i$ 作为输入，然后输出各自的集成向量 $\tilde{h}_t^i$。

- 注意如果某个智能体在多个 communication group 中，那么该智能体在当前通信交流中的集成向量 $\tilde{h}_t^i$ 输出将作为它在下一个 communication group 中的隐藏层编码向量输入。这种处于交界处的智能体就会起到一个信息桥梁的作用，使得通信信息可以在不同的 communication group 之间进行传播。
- 为了解决不同智能体角色和异构智能体的问题，所以双向 LSTM 网络中智能体的输入顺序是固定。

<img width="50%" src="/img/in-post/2020-06-23-强化学习论文（11）ATOC.assets/image-20200623163109296.png"/>



5、接着在 ActorNet (2) 中，将该智能体的隐藏层编码向量 $h_t^i$ 和其集成向量 $\tilde{h}_t^i$ 作为输入，然后输出其动作。

<img width="30%" src="/img/in-post/2020-06-23-强化学习论文（11）ATOC.assets/image-20200623163823318.png"/>



### 训练过程

1、crtic 网络损失函数

<img width="80%" src="/img/in-post/2020-06-23-强化学习论文（11）ATOC.assets/image-20200623165335077.png"/>



2、actor 网络求导

<img width="70%" src="/img/in-post/2020-06-23-强化学习论文（11）ATOC.assets/image-20200623165605560.png"/>



3、communication channel 网络求导

<img width="70%" src="/img/in-post/2020-06-23-强化学习论文（11）ATOC.assets/image-20200623165703005.png"/>



4、attention unit 作为一个二分类器，不通过强化学习进行训练，而是单独收集训练数据通过监督学习进行训练。

- 训练数据：对于每一个 initiator 及其 communication group，我们计算其 communication group 中每一个智能体采用协作动作（即使用集成向量作为额外输入得到的动作）与不采用协作动作在 Q 值上带来的差值的平均值 $\Delta Q_i$，然后将 $(\Delta Q_i, h^i)$ 作为一个训练样本。

<img width="60%" src="/img/in-post/2020-06-23-强化学习论文（11）ATOC.assets/image-20200623170119480.png"/>

- 损失函数采用二分类交叉熵损失函数

<img width="80%" src="/img/in-post/2020-06-23-强化学习论文（11）ATOC.assets/image-20200623170218139.png"/>



### 算法伪代码

ATOC 算法伪代码

<img width="100%" src="/img/in-post/2020-06-23-强化学习论文（11）ATOC.assets/image-20200623155748650.png"/>



## 实验

1、在三个协作环境上进行实验，其中的智能体规模都较大

- cooperative navigation：$N = 50, L = 50$、$N = 100, L = 100$
- cooperative pushball：$N = 50$
- predator-prey：60 slower predators chase 20 faster preys

<img width="100%" src="/img/in-post/2020-06-23-强化学习论文（11）ATOC.assets/image-20200623160126416.png"/>



2、CommNet 和 BiCNet 通信协作效果差的原因如下

- 在大规模智能体问题中 CommNet 对所有智能体的通信信息进行平均，而其中会包含很多无效信息，这样便无法区分其中有价值的信息。
- 对于 BiCNet 来说，RNN 可以认为是一种加权平均的方式，但是随着智能体的增多，RNN 也无法捕获到不同智能体之间信息的重要程度。
- 而 ATOC 可以动态决定是否进行通信，而且其通信信息都来源于邻近的智能体，对于决策的帮助较大。



3、在实验中发现训练好 ATOC 之后去除通信交流的模块其表现仍优于各个 baseline 算法，说明在训练时合作策略梯度通过反向传播更新单个策略网络，这使得代理适当地学会了在无需通信的情况下推断其他代理的行为，从而可以协同工作。



## 其他补充

1、在智能体视野范围内选择其他的智能体作为 collaborators 有以下好处：

- 邻近的智能体的局部观察更加相似，因而更容易互相理解。
- 邻近的智能体之间更容易协作。
- 所有智能体之间共享相同的 actor 网络，所以邻近的智能体之间的动作更加相似，但交流可以提高它们策略的复杂程度。



2、相比于 BiCNet 使用双向 RNN 作为通信信道，论文使用双向 LSTM 可以有选择地输出促进合作的信息，而通过 gates 忽略阻碍合作的信息。



3、虽然该算法实现了智能体自己选择是否进行通信，但是其实通信的范围及智能体是预定义好的，如果进行通信就只与周围的几个智能体进行通信，所以并没有实现如何选择智能体进行通信这一步。



## 参考资料及致谢

所有参考资料在《强化学习思考（1）前言》中已列出，再次向强化学习的大佬前辈们表达衷心的感谢！

