---
layout:     post
title:      "强化学习思考（2）强化学习简介"
subtitle:    "强化学习简介"
date:       2020-04-12 10:00:00
author:     Shunyu
header-img: img/post-bg-2015.jpg
header-mask: 0.1
catalog: true
mathjax: true
tags:
    - 强化学习
    - 强化学习思考
---



关于强化学习简介的注意事项。



## 目录

- [强化学习思考（1）前言](https://liushunyu.github.io/2020/04/12/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E6%80%9D%E8%80%83-1-%E5%89%8D%E8%A8%80/)
- [强化学习思考（2）强化学习简介](https://liushunyu.github.io/2020/04/12/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E6%80%9D%E8%80%83-2-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/)
- [强化学习思考（3）马尔可夫决策过程](https://liushunyu.github.io/2020/04/12/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E6%80%9D%E8%80%83-3-%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B/)
- [强化学习思考（4）模仿学习和监督学习](https://liushunyu.github.io/2020/04/13/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E6%80%9D%E8%80%83-4-%E6%A8%A1%E4%BB%BF%E5%AD%A6%E4%B9%A0%E5%92%8C%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/)
- [强化学习思考（5）动态规划](https://liushunyu.github.io/2020/04/15/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E6%80%9D%E8%80%83-5-%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/)
- [强化学习思考（6）蒙特卡罗和时序差分](https://liushunyu.github.io/2020/04/15/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E6%80%9D%E8%80%83-6-%E8%92%99%E7%89%B9%E5%8D%A1%E7%BD%97%E5%92%8C%E6%97%B6%E5%BA%8F%E5%B7%AE%E5%88%86/)
- [强化学习思考（7）策略梯度](https://liushunyu.github.io/2020/04/18/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E6%80%9D%E8%80%83-7-%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6/)



## Reward

Reward 是强化学习的核心，强化学习问题中可以没有 state / observation，但是不能没有 reward。（如多臂赌博机问题）

### Reward Hypothesis

>Definition (Reward Hypothesis):
>
>All goals can be described by the maximisation of expected cumulative reward.

如果一个问题不满足奖励假设，那么就不能用强化学习去解决。



### expected reward

In RL, we almost always care about expectations:

- because $r(s,a)$ is not smooth

- if $\pi_\theta(a \mid s)=\alpha$, $\Bbb{E}[r(s,a)] = \alpha$ is smooth



## Model and Environment

Model 会预测 environment 接下来的输出，其定义如下：



$$
P_{ss'}^a = P[S_{t+1} = s' | S_t = s, A_t = a] \\
R_{s}^a =E[R_{t+1} | S_t = s, A_t = a]
$$



## 强化学习分类

##### Model Free / Model Based

- Model Free：environment 的 model 对于 agent 来说不是已知的。

- Model Based：environment 的 model 对于 agent 来说是已知的。



##### Prediction / Control

强化学习算法通常分为 Prediction 和 Control 两种

- Prediction: evaluate the future with a given policy. (evaluation)

- Control: Find the best policy to optimise the future. (evaluation + improvement)



##### Learning / Planning

- Reinforcement Learning:
  - The environment is initially unknown
  - The agent interacts with the environment
  - The agent improves its policy

- Planning:
  - A model of the environment is known
  - The agent performs computations with its model (without any external interaction)
  - The agent improves its policy
  - a.k.a. deliberation, reasoning, introspection, pondering, thought, search



##### Exploration / Exploitation

- Exploration finds more information about the environment
- Exploitation exploits known information to maximise reward



##### Value Based / Policy Based

- Value Based 用于决策（选择 action）的函数是 value function。

- Policy Based 用于决策（选择 action）的函数是 policy function。



##### Off-Policy / On-Policy

- Off policy: 
  - able to improve the policy without generating new samples from that policy
  - Off-Policy 学习的 agent 以及和环境进行互动的 agent 是**不同的agent**。
- On policy: 
  - each time the policy is changed, even a little bit, we need to generate new samples
  -  On-Policy 学习的 agent 以及和环境进行互动的 agent 是**相同的agent**。



## 强化学习框架

一般强化学习算法遵循这三步：

1、generate samples

2、fit a model / estimate the return

3、improve the policy



### fit a model / estimate the return

通常有下面三种：

- learn $p(s_{t+1} \mid s_t,a_t)$
- evaluate returns $R_\tau=\sum_t r(s_t,a_t)$
- fit $V(s)$ or $Q(s,a)$



### improve the policy

通常有下面两种：

- Value-based: if we have policy $\pi$, and we know Q"(s, a), then we can improve $\pi$
  - set $\pi'(a \mid s)=1$ if $a=\arg \max_a Q^\pi(s,a)$
  - this policy is at least as good as $\pi$ and it doesn't matter what $\pi$ is 

- Policy gradients: compute gradient to increase probability of good actions $a$
  - modify $\pi(a \mid s)$ to increase probability of $a$ if $Q^\pi(s,a)>V^\pi(s)$
  - if $Q^\pi(s,a)>V^\pi(s)$, then $a$ is better than average



### 常见算法

- Policy gradients: directly differentiate the above objective

- Value-based: estimate value function or Q-function of the optimal policy (no explicit policy)

- Actor-critic: estimate value function or Q-function of the current policy, use it to improve policy

- Model-based RL: estimate the transition model, and then
  - Use the model to plan (no explicit policy)
    - Trajectory optimization/optimal control (primarily in continuous spaces) – essentially backpropagation to optimize over actions
    - Discrete planning in discrete action spaces – e.g., Monte Carlo tree search
  - Use it to improve a policy
    - Backpropagate gradients into the policy
    - Requires some tricks to make it work
  - Use the model to learn a value function
    - Dynamic programming
    - Generate simulated experience for model-free learner



## Comparison

### Comparison: sample efficiency

<img width="480" src="/img/in-post/2020-04-12-强化学习思考（2）强化学习简介.assets/image-20200413162112420.png"/>



### Comparison: stability and ease of use

#### SL and RL 算法比较

**Supervised learning**

- almost always gradient descent

**Reinforcement learning**

- often not gradient descent
  - Policy gradient: is gradient descent, but also often the least efficient!
  - Q-learning: fixed point iteration
  - Model-based RL: model is not optimized for expected reward



#### RL 各算法比较

**Policy gradient**

- The only one that actually performs gradient descent (ascent) on the true objective

**Value function fitting**

- At best, minimizes error of fit (“Bellman error”)
  - Not the same as expected reward
- At worst, doesn’t optimize anything
  - Many popular deep RL value fitting algorithms are not guaranteed to converge to anything in the nonlinear case

**Model-based RL**

- Model minimizes error of fit
  - This will converge
- No guarantee that better model = better policy



### Comparison: assumptions

- Common assumption #1: full observability
  - Generally assumed by value function fitting methods
  - Can be mitigated by adding recurrence
- Common assumption #2: episodic learning
  - Often assumed by pure policy gradient methods
  - Assumed by some model-based RL methods
- Common assumption #3: continuity or smoothness
  - Assumed by some continuous value function learning methods
  - Often assumed by some model-based RL methods




## 参考资料及致谢

所有参考资料在前言中已列出，再次向强化学习的大佬前辈们表达衷心的感谢！
