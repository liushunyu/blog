---

layout:     post
title:      "强化学习论文（8）VDN"
subtitle:   "Value-Decomposition Networks For Cooperative Multi-Agent Learning"
date:       2020-06-17 09:00:00
author:     Shunyu
header-img: img/post-bg-2015.jpg
header-mask: 0.1
catalog: true
mathjax: true
tags:
    - 强化学习
    - 强化学习论文
---



**标签：**

VDN; value-based; off-policy; model-free; communication; continuous communication channel; discrete action space; continuous state space; cooperative task; credit assignment; centralized training with decentralized execution; value function factorization; multi-agent;



[论文链接](https://arxiv.org/abs/1706.05296)



## 创新点及贡献

1、基于 DRQN 提出了值分解网络架构，可以在仅有全局收益的情况下学习不同智能体自己的动作价值函数，从而解决由于部分可观察导致的伪收益（spuriou reward）和懒惰智能体（lazy agent）问题。

- 虽然本文没有提及，但其实本质上是解决了多智能体的信用分配问题。



## 研究痛点

1、伪收益（spuriou reward）：在独立式学习的方法中从单个智能体的角度来看，环境是部分观察的，所以智能体可能会收到来自队友（未观察到的）动作的虚假收益信号，独立式学习的方法无法区分队友的探索与环境的随机性。

2、懒惰智能体（lazy agent）：假设只有两个智能体，在集中式学习的方法中可能会导致只有一个智能体是活跃的，另一个智能体是懒惰的。当一个智能体学习了有用的策略，但不鼓励第二个智能体学习时，就会发生这种情况，因为其探索会阻碍第一个智能体并导致较差的团队奖励。



## 算法流程

1、VDN 框架图如下

<img width="80%" src="/img/in-post/2020-06-17-强化学习论文（8）VDN.assets/image-20200617164918915.png"/>



2、VDN + High+Low-level communication 框架图如下

<img width="80%" src="/img/in-post/2020-06-17-强化学习论文（8）VDN.assets/image-20200617165007180.png"/>



### 主要思路

1、每个智能体采用 DRQN 实现，其 $Q$ 值输出会合并为一个团队的 $Q$ 值，从而实现值分解网络的设计。

<img width="70%" src="/img/in-post/2020-06-17-强化学习论文（8）VDN.assets/image-20200617165543079.png"/>



2、除了基础的 DRQN 架构还使用了以下方法：

- dueling architecture
- multi-step updates with a forward view eligibility trace



3、如果智能体之间需要进行通信，我们分别采取 High level 通信和 Low level 通信两种方式：

- High level 通信：将 LSTM 层的输出进行 concatenation
- Low level 通信：将全连接层层的输出进行 concatenation



4、所有智能体的网络进行参数共享，在输入中需要提供智能体的身份信息，以保证 Agent Invariance。



## 实验

1、在三种不同的 maze 环境中进行各种消融实验

<img width="100%" src="/img/in-post/2020-06-17-强化学习论文（8）VDN.assets/image-20200617170345919.png"/>



2、实验中发现权重共享对于不同智能体具有较大不同 reward 幅度差异的环境是有问题的。



## 其他补充

1、本文提出的伪收益（spuriou reward）和懒惰智能体（lazy agent）问题这个角度很不错，虽然其本质是信用分配问题，但是这种说法很新颖而且直觉上接受度比较高。

2、仍然是不适合大规模智能体的办法。

3、局限于只能适用于离散动作空间。



## 参考资料及致谢

所有参考资料在《强化学习思考（1）前言》中已列出，再次向强化学习的大佬前辈们表达衷心的感谢！

